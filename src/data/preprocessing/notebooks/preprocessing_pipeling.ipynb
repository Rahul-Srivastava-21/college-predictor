{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01776026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries imported successfully!\n",
      "✓ Ready for data preprocessing\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"✓ All libraries imported successfully!\")\n",
    "print(\"✓ Ready for data preprocessing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "162b7f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset loaded successfully!\n",
      "Dataset shape: (309145, 8)\n",
      "Columns: ['College_Code', 'College_Name', 'Category', 'Branch', 'Cutoff_Rank', 'Year', 'Round', 'Exam_Type']\n",
      "\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>College_Code</th>\n",
       "      <th>College_Name</th>\n",
       "      <th>Category</th>\n",
       "      <th>Branch</th>\n",
       "      <th>Cutoff_Rank</th>\n",
       "      <th>Year</th>\n",
       "      <th>Round</th>\n",
       "      <th>Exam_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E001</td>\n",
       "      <td>Acharya Institute of Technology</td>\n",
       "      <td>GM</td>\n",
       "      <td>AE-Aeronautical Engineering</td>\n",
       "      <td>63649.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>COMEDK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E001</td>\n",
       "      <td>Acharya Institute of Technology</td>\n",
       "      <td>GM</td>\n",
       "      <td>AI-Artificial Intelligence &amp; Machine Learning</td>\n",
       "      <td>27873.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>COMEDK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E001</td>\n",
       "      <td>Acharya Institute of Technology</td>\n",
       "      <td>GM</td>\n",
       "      <td>BT-Biotechnology</td>\n",
       "      <td>46368.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>COMEDK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E001</td>\n",
       "      <td>Acharya Institute of Technology</td>\n",
       "      <td>GM</td>\n",
       "      <td>CD-Computer Science &amp; Engineering (Data Science)</td>\n",
       "      <td>24270.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>COMEDK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E001</td>\n",
       "      <td>Acharya Institute of Technology</td>\n",
       "      <td>GM</td>\n",
       "      <td>CS-Computer Science &amp; Engineering</td>\n",
       "      <td>20937.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>COMEDK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  College_Code                     College_Name Category                                            Branch  Cutoff_Rank  Year  Round Exam_Type\n",
       "0         E001  Acharya Institute of Technology       GM                       AE-Aeronautical Engineering      63649.0  2023      4    COMEDK\n",
       "1         E001  Acharya Institute of Technology       GM     AI-Artificial Intelligence & Machine Learning      27873.0  2023      4    COMEDK\n",
       "2         E001  Acharya Institute of Technology       GM                                  BT-Biotechnology      46368.0  2023      4    COMEDK\n",
       "3         E001  Acharya Institute of Technology       GM  CD-Computer Science & Engineering (Data Science)      24270.0  2023      4    COMEDK\n",
       "4         E001  Acharya Institute of Technology       GM                 CS-Computer Science & Engineering      20937.0  2023      4    COMEDK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load your master CSV file\n",
    "# Replace 'your_file_path.csv' with the actual path to your dataset\n",
    "file_path = 'combined_cutoffs.csv'  # UPDATE THIS PATH\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"✓ Dataset loaded successfully!\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    display(df.head())\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"❌ File not found. Please update the file_path variable with correct path.\")\n",
    "    print(\"Expected columns: College_Code, College_Name, Category, Branch, Cutoff_Rank, Year, Round, Exam_Type\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading dataset: {str(e)}\")\n",
    "    print(\"Please check your file path and file format.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea0acdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASET OVERVIEW ===\n",
      "Shape: (309145, 8)\n",
      "Memory usage: 115870.97 KB\n",
      "\n",
      "=== DATA TYPES ===\n",
      "College_Code     object\n",
      "College_Name     object\n",
      "Category         object\n",
      "Branch           object\n",
      "Cutoff_Rank     float64\n",
      "Year              int64\n",
      "Round             int64\n",
      "Exam_Type        object\n",
      "dtype: object\n",
      "\n",
      "=== MISSING VALUES ANALYSIS ===\n",
      "Missing values summary:\n",
      "                  Column  Missing_Count  Missing_Percentage Data_Type\n",
      "Category        Category           3287            1.063255    object\n",
      "Branch            Branch           3067            0.992091    object\n",
      "Cutoff_Rank  Cutoff_Rank           3067            0.992091   float64\n",
      "\n",
      "=== UNIQUE VALUES PER COLUMN ===\n",
      "College_Code: 347 unique values\n",
      "College_Name: 822 unique values\n",
      "Category: 65 unique values\n",
      "Branch: 490 unique values\n",
      "Cutoff_Rank: 118830 unique values\n",
      "Year: 5 unique values\n",
      "Round: 5 unique values\n",
      "  Values: [0, 1, 2, 3, 4]\n",
      "Exam_Type: 2 unique values\n",
      "  Values: ['CET', 'COMEDK']\n",
      "\n",
      "=== BASIC STATISTICS FOR NUMERICAL COLUMNS ===\n",
      "         Cutoff_Rank           Year          Round\n",
      "count  306078.000000  309145.000000  309145.000000\n",
      "mean    81657.597014    2022.345498       1.372505\n",
      "std     55585.078884       1.363919       1.086947\n",
      "min        18.000000    2020.000000       0.000000\n",
      "25%     37983.000000    2021.000000       0.000000\n",
      "50%     71246.000000    2022.000000       1.000000\n",
      "75%    115069.000000    2024.000000       2.000000\n",
      "max    274884.000000    2024.000000       4.000000\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive data exploration\n",
    "print(\"=== DATASET OVERVIEW ===\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "\n",
    "print(\"\\n=== DATA TYPES ===\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\n=== MISSING VALUES ANALYSIS ===\")\n",
    "missing_data = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': df.isnull().sum(),\n",
    "    'Missing_Percentage': (df.isnull().sum() / len(df)) * 100,\n",
    "    'Data_Type': df.dtypes\n",
    "})\n",
    "\n",
    "missing_data = missing_data[missing_data['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
    "print(\"Missing values summary:\")\n",
    "print(missing_data)\n",
    "\n",
    "if len(missing_data) == 0:\n",
    "    print(\"✓ No missing values found!\")\n",
    "\n",
    "print(\"\\n=== UNIQUE VALUES PER COLUMN ===\")\n",
    "for col in df.columns:\n",
    "    unique_count = df[col].nunique()\n",
    "    print(f\"{col}: {unique_count} unique values\")\n",
    "    if unique_count <= 20 and col in ['Category', 'Round', 'Exam_Type']:\n",
    "        print(f\"  Values: {sorted(df[col].dropna().unique())}\")\n",
    "    elif col == 'Branch' and unique_count <= 50:\n",
    "        branch_values = sorted(df[col].dropna().unique())\n",
    "        print(f\"  Sample branches: {branch_values[:10]}...\")\n",
    "        if len(branch_values) > 10:\n",
    "            print(f\"  Total branches: {len(branch_values)}\")\n",
    "\n",
    "print(\"\\n=== BASIC STATISTICS FOR NUMERICAL COLUMNS ===\")\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48feedfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Missing values handling function defined\n"
     ]
    }
   ],
   "source": [
    "def handle_missing_values_admission_data(df):\n",
    "    \"\"\"\n",
    "    Handle missing values specifically for college admission dataset\n",
    "    Adapted for your specific data structure\n",
    "    \"\"\"\n",
    "    df_cleaned = df.copy()\n",
    "    \n",
    "    print(\"=== MISSING VALUES HANDLING STRATEGY ===\\n\")\n",
    "    \n",
    "    # Check which columns actually have missing values\n",
    "    missing_cols = df_cleaned.isnull().sum()\n",
    "    missing_cols = missing_cols[missing_cols > 0]\n",
    "    \n",
    "    if len(missing_cols) == 0:\n",
    "        print(\"✓ No missing values found!\")\n",
    "        return df_cleaned\n",
    "    \n",
    "    print(f\"Columns with missing values: {list(missing_cols.index)}\")\n",
    "    \n",
    "    # 1. Handle Category missing values\n",
    "    if 'Category' in missing_cols.index:\n",
    "        print(\"\\n1. Handling Category:\")\n",
    "        category_missing = df_cleaned['Category'].isnull()\n",
    "        \n",
    "        # Check most common category in your dataset\n",
    "        most_common_category = df_cleaned['Category'].mode().iloc[0] if len(df_cleaned['Category'].mode()) > 0 else 'GM'\n",
    "        \n",
    "        # Fill missing categories with most common (likely 'GM' for General Merit)\n",
    "        df_cleaned['Category'] = df_cleaned['Category'].fillna(most_common_category)\n",
    "        print(f\"   Filled {category_missing.sum()} missing Categories with '{most_common_category}'\")\n",
    "        \n",
    "        # Display category distribution\n",
    "        print(\"   Top 10 categories in dataset:\")\n",
    "        print(df_cleaned['Category'].value_counts().head(10))\n",
    "    \n",
    "    # 2. Handle Branch missing values\n",
    "    if 'Branch' in missing_cols.index:\n",
    "        print(\"\\n2. Handling Branch:\")\n",
    "        branch_missing_before = df_cleaned['Branch'].isnull().sum()\n",
    "        \n",
    "        # For missing branches, use most common branch from same college\n",
    "        for idx in df_cleaned[df_cleaned['Branch'].isnull()].index:\n",
    "            college_code = df_cleaned.loc[idx, 'College_Code']\n",
    "            same_college_branches = df_cleaned[\n",
    "                (df_cleaned['College_Code'] == college_code) & \n",
    "                (df_cleaned['Branch'].notna())\n",
    "            ]['Branch']\n",
    "            \n",
    "            if len(same_college_branches) > 0:\n",
    "                # Use most common branch in same college\n",
    "                common_branch = same_college_branches.mode().iloc[0]\n",
    "                df_cleaned.loc[idx, 'Branch'] = common_branch\n",
    "            else:\n",
    "                # Use overall most common branch\n",
    "                overall_common = df_cleaned['Branch'].mode().iloc[0] if len(df_cleaned['Branch'].mode()) > 0 else 'CS'\n",
    "                df_cleaned.loc[idx, 'Branch'] = overall_common\n",
    "        \n",
    "        branch_missing_after = df_cleaned['Branch'].isnull().sum()\n",
    "        print(f\"   Filled {branch_missing_before - branch_missing_after} missing Branches\")\n",
    "        \n",
    "        # Show top branches\n",
    "        print(\"   Top 10 branches in dataset:\")\n",
    "        print(df_cleaned['Branch'].value_counts().head(10))\n",
    "    \n",
    "    # 3. Handle Cutoff_Rank missing values\n",
    "    if 'Cutoff_Rank' in missing_cols.index:\n",
    "        print(\"\\n3. Handling Cutoff_Rank:\")\n",
    "        rank_missing_before = df_cleaned['Cutoff_Rank'].isnull().sum()\n",
    "        \n",
    "        # Strategy: Use median rank for similar records\n",
    "        for idx in df_cleaned[df_cleaned['Cutoff_Rank'].isnull()].index:\n",
    "            branch = df_cleaned.loc[idx, 'Branch']\n",
    "            category = df_cleaned.loc[idx, 'Category']\n",
    "            exam_type = df_cleaned.loc[idx, 'Exam_Type']\n",
    "            year = df_cleaned.loc[idx, 'Year']\n",
    "            \n",
    "            # Find similar records (same branch, category, exam type, year)\n",
    "            similar_records = df_cleaned[\n",
    "                (df_cleaned['Branch'] == branch) & \n",
    "                (df_cleaned['Category'] == category) & \n",
    "                (df_cleaned['Exam_Type'] == exam_type) & \n",
    "                (df_cleaned['Year'] == year) &\n",
    "                (df_cleaned['Cutoff_Rank'].notna())\n",
    "            ]['Cutoff_Rank']\n",
    "            \n",
    "            if len(similar_records) > 0:\n",
    "                fill_value = similar_records.median()\n",
    "                df_cleaned.loc[idx, 'Cutoff_Rank'] = fill_value\n",
    "            else:\n",
    "                # Fallback: Use branch median for same year and exam type\n",
    "                branch_similar = df_cleaned[\n",
    "                    (df_cleaned['Branch'] == branch) & \n",
    "                    (df_cleaned['Year'] == year) &\n",
    "                    (df_cleaned['Exam_Type'] == exam_type) &\n",
    "                    (df_cleaned['Cutoff_Rank'].notna())\n",
    "                ]['Cutoff_Rank']\n",
    "                \n",
    "                if len(branch_similar) > 0:\n",
    "                    df_cleaned.loc[idx, 'Cutoff_Rank'] = branch_similar.median()\n",
    "                else:\n",
    "                    # Final fallback: Overall median\n",
    "                    overall_median = df_cleaned['Cutoff_Rank'].median()\n",
    "                    df_cleaned.loc[idx, 'Cutoff_Rank'] = overall_median\n",
    "        \n",
    "        rank_missing_after = df_cleaned['Cutoff_Rank'].isnull().sum()\n",
    "        print(f\"   Filled {rank_missing_before - rank_missing_after} missing Cutoff_Ranks\")\n",
    "    \n",
    "    # 4. Handle any other missing values\n",
    "    remaining_missing = df_cleaned.isnull().sum()\n",
    "    remaining_missing = remaining_missing[remaining_missing > 0]\n",
    "    \n",
    "    if len(remaining_missing) > 0:\n",
    "        print(f\"\\n4. Remaining missing values:\")\n",
    "        print(remaining_missing)\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "print(\"✓ Missing values handling function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbfb8c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APPLYING MISSING VALUES HANDLING:\n",
      "==================================================\n",
      "Original dataset:\n",
      "  Shape: (309145, 8)\n",
      "  Total missing values: 9421\n",
      "=== MISSING VALUES HANDLING STRATEGY ===\n",
      "\n",
      "Columns with missing values: ['Category', 'Branch', 'Cutoff_Rank']\n",
      "\n",
      "1. Handling Category:\n",
      "   Filled 3287 missing Categories with 'GM'\n",
      "   Top 10 categories in dataset:\n",
      "Category\n",
      "GM     35884\n",
      "2AG    19795\n",
      "SCG    19657\n",
      "GMR    19304\n",
      "3BG    17842\n",
      "GMH    17087\n",
      "3AG    16995\n",
      "2BG    16943\n",
      "1G     16362\n",
      "STG    15188\n",
      "Name: count, dtype: int64\n",
      "\n",
      "2. Handling Branch:\n",
      "   Filled 3067 missing Branches\n",
      "   Top 10 branches in dataset:\n",
      "Branch\n",
      "CS Computers                    70033\n",
      "EC Electronics                  51208\n",
      "IE Info.Science                 27884\n",
      "EE Electrical                   19907\n",
      "CE Civil                        18614\n",
      "AI Artificial Intelligence      17412\n",
      "ME Mechanical                   13602\n",
      "CA CS (AI, Machine Learning)     8189\n",
      "DS Comp. Sc. Engg- Data Sc.      7638\n",
      "AD Artificial Intel, Data Sc     6626\n",
      "Name: count, dtype: int64\n",
      "\n",
      "3. Handling Cutoff_Rank:\n",
      "   Filled 3067 missing Cutoff_Ranks\n",
      "\n",
      "=== MISSING VALUES HANDLING SUMMARY ===\n",
      "Original missing values: 9421\n",
      "Remaining missing values: 0\n",
      "Missing values resolved: 9421\n",
      "\n",
      "Missing values by column after cleaning:\n",
      "College_Code    0\n",
      "College_Name    0\n",
      "Category        0\n",
      "Branch          0\n",
      "Cutoff_Rank     0\n",
      "Year            0\n",
      "Round           0\n",
      "Exam_Type       0\n",
      "dtype: int64\n",
      "\n",
      "✅ All missing values handled successfully!\n",
      "\n",
      "✅ Dataset ready for feature engineering\n",
      "Cleaned dataset shape: (309145, 8)\n"
     ]
    }
   ],
   "source": [
    "# Apply missing values handling to your dataset\n",
    "print(\"APPLYING MISSING VALUES HANDLING:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Store original data info for comparison\n",
    "original_shape = df.shape\n",
    "original_missing = df.isnull().sum().sum()\n",
    "\n",
    "print(f\"Original dataset:\")\n",
    "print(f\"  Shape: {original_shape}\")\n",
    "print(f\"  Total missing values: {original_missing}\")\n",
    "\n",
    "# Apply the function\n",
    "df_cleaned = handle_missing_values_admission_data(df)\n",
    "\n",
    "# Summary of changes\n",
    "cleaned_missing = df_cleaned.isnull().sum().sum()\n",
    "\n",
    "print(f\"\\n=== MISSING VALUES HANDLING SUMMARY ===\")\n",
    "print(f\"Original missing values: {original_missing}\")\n",
    "print(f\"Remaining missing values: {cleaned_missing}\")\n",
    "print(f\"Missing values resolved: {original_missing - cleaned_missing}\")\n",
    "\n",
    "print(\"\\nMissing values by column after cleaning:\")\n",
    "missing_summary = df_cleaned.isnull().sum()\n",
    "print(missing_summary)\n",
    "\n",
    "if cleaned_missing == 0:\n",
    "    print(f\"\\n✅ All missing values handled successfully!\")\n",
    "else:\n",
    "    print(f\"\\n⚠️ {cleaned_missing} missing values remain\")\n",
    "\n",
    "print(f\"\\n✅ Dataset ready for feature engineering\")\n",
    "print(f\"Cleaned dataset shape: {df_cleaned.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14c1b94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEATURE ENGINEERING ===\n",
      "1. Creating Rank_Percentile feature:\n",
      "   ✓ Rank_Percentile: Normalized rank within year and exam type (0-1 scale)\n",
      "\n",
      "2. Creating Years_Since_2020 feature:\n",
      "   ✓ Years_Since_2020: Temporal trend encoding\n",
      "\n",
      "3. Creating Branch_Popularity_Score:\n",
      "   ✓ Branch_Popularity_Score: Ranked 490 branches by median cutoff rank\n",
      "   Top 5 most popular branches (lowest cutoff): ['Computer\\rScience and\\rEngineering\\r(Block Chain)', 'Computer Science\\rand Engineering\\r(Cyber Security)', 'Electronics &\\rInstrumentat\\rion\\rEngineering', 'Computer Science\\rand Business\\rSystems', 'Computer Science\\rand Business Systems']\n",
      "\n",
      "4. Creating College_Selectivity:\n",
      "   ✓ College_Selectivity: Median cutoff rank per college\n",
      "\n",
      "5. Creating Round_Difficulty:\n",
      "   ✓ Round_Difficulty: Round progression indicator\n",
      "\n",
      "6. Creating Category_Main:\n",
      "   ✓ Category_Main: Simplified category grouping\n",
      "   Category distribution: {'Reserved': 167204, 'General': 93208, 'Other': 25346, 'ST': 23387}\n",
      "\n",
      "=== FEATURE ENGINEERING SUMMARY ===\n",
      "Original features: 8\n",
      "After feature engineering: 14\n",
      "New features added: 6\n",
      "New feature names: ['Rank_Percentile', 'Years_Since_2020', 'Branch_Popularity_Score', 'College_Selectivity', 'Round_Difficulty', 'Category_Main']\n",
      "\n",
      "Sample of engineered features (first 5 rows):\n",
      "  College_Code                                            Branch  Cutoff_Rank  Rank_Percentile  Years_Since_2020  Branch_Popularity_Score  College_Selectivity  Round_Difficulty Category_Main\n",
      "0         E001                       AE-Aeronautical Engineering      63649.0         0.742270                 3                      302              26819.0                 4       General\n",
      "1         E001     AI-Artificial Intelligence & Machine Learning      27873.0         0.324862                 3                      116              26819.0                 4       General\n",
      "2         E001                                  BT-Biotechnology      46368.0         0.545532                 3                      238              26819.0                 4       General\n",
      "3         E001  CD-Computer Science & Engineering (Data Science)      24270.0         0.284625                 3                      102              26819.0                 4       General\n",
      "4         E001                 CS-Computer Science & Engineering      20937.0         0.243117                 3                      248              26819.0                 4       General\n",
      "\n",
      "✅ Feature engineering completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering - Create additional useful features\n",
    "print(\"=== FEATURE ENGINEERING ===\")\n",
    "\n",
    "df_features = df_cleaned.copy()\n",
    "\n",
    "# 1. Rank Percentile - Normalized rank within year/exam type\n",
    "print(\"1. Creating Rank_Percentile feature:\")\n",
    "df_features['Rank_Percentile'] = df_features.groupby(['Year', 'Exam_Type'])['Cutoff_Rank'].rank(pct=True)\n",
    "print(\"   ✓ Rank_Percentile: Normalized rank within year and exam type (0-1 scale)\")\n",
    "\n",
    "# 2. Years since 2020 - Temporal encoding\n",
    "print(\"\\n2. Creating Years_Since_2020 feature:\")\n",
    "df_features['Years_Since_2020'] = df_features['Year'] - 2020\n",
    "print(\"   ✓ Years_Since_2020: Temporal trend encoding\")\n",
    "\n",
    "# 3. Branch popularity score (based on cutoff rank - lower rank = more popular)\n",
    "print(\"\\n3. Creating Branch_Popularity_Score:\")\n",
    "branch_popularity = df_features.groupby('Branch')['Cutoff_Rank'].median().sort_values()\n",
    "branch_popularity_rank = {branch: rank for rank, branch in enumerate(branch_popularity.index)}\n",
    "df_features['Branch_Popularity_Score'] = df_features['Branch'].map(branch_popularity_rank)\n",
    "print(f\"   ✓ Branch_Popularity_Score: Ranked {len(branch_popularity_rank)} branches by median cutoff rank\")\n",
    "print(f\"   Top 5 most popular branches (lowest cutoff): {list(branch_popularity.head().index)}\")\n",
    "\n",
    "# 4. College selectivity (average cutoff rank per college - lower = more selective)\n",
    "print(\"\\n4. Creating College_Selectivity:\")\n",
    "college_selectivity = df_features.groupby('College_Code')['Cutoff_Rank'].median()\n",
    "df_features['College_Selectivity'] = df_features['College_Code'].map(college_selectivity)\n",
    "print(f\"   ✓ College_Selectivity: Median cutoff rank per college\")\n",
    "\n",
    "# 5. Round difficulty (based on your explanation)\n",
    "print(\"\\n5. Creating Round_Difficulty:\")\n",
    "# Higher rounds typically have higher (worse) cutoff ranks\n",
    "round_difficulty_map = {0: 0, 1: 1, 2: 2, 3: 3, 4: 4}  # Direct mapping for now\n",
    "df_features['Round_Difficulty'] = df_features['Round'].map(round_difficulty_map)\n",
    "print(\"   ✓ Round_Difficulty: Round progression indicator\")\n",
    "\n",
    "# 6. Category type simplification (extract main category)\n",
    "print(\"\\n6. Creating Category_Main:\")\n",
    "def extract_main_category(cat):\n",
    "    if 'GM' in str(cat):\n",
    "        return 'General'\n",
    "    elif any(x in str(cat) for x in ['SC', '2A', '2B', '3A', '3B']):\n",
    "        return 'Reserved'\n",
    "    elif 'ST' in str(cat):\n",
    "        return 'ST'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "df_features['Category_Main'] = df_features['Category'].apply(extract_main_category)\n",
    "print(\"   ✓ Category_Main: Simplified category grouping\")\n",
    "print(f\"   Category distribution: {df_features['Category_Main'].value_counts().to_dict()}\")\n",
    "\n",
    "print(f\"\\n=== FEATURE ENGINEERING SUMMARY ===\")\n",
    "print(f\"Original features: {df_cleaned.shape[1]}\")\n",
    "print(f\"After feature engineering: {df_features.shape[1]}\")\n",
    "print(f\"New features added: {df_features.shape[1] - df_cleaned.shape[1]}\")\n",
    "new_features = [col for col in df_features.columns if col not in df_cleaned.columns]\n",
    "print(f\"New feature names: {new_features}\")\n",
    "\n",
    "# Display sample of new features\n",
    "print(\"\\nSample of engineered features (first 5 rows):\")\n",
    "sample_cols = ['College_Code', 'Branch', 'Cutoff_Rank', 'Rank_Percentile', 'Years_Since_2020', \n",
    "               'Branch_Popularity_Score', 'College_Selectivity', 'Round_Difficulty', 'Category_Main']\n",
    "print(df_features[sample_cols].head())\n",
    "\n",
    "print(f\"\\n✅ Feature engineering completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3572c760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Categorical encoding function defined for your dataset\n"
     ]
    }
   ],
   "source": [
    "def encode_categorical_features(df, encoding_strategy='comprehensive'):\n",
    "    \"\"\"\n",
    "    Apply appropriate categorical encoding for college admission prediction\n",
    "    Adapted for your specific dataset with 490 branches and detailed categories\n",
    "    \"\"\"\n",
    "    df_encoded = df.copy()\n",
    "    encoders = {}\n",
    "    \n",
    "    print(\"=== CATEGORICAL ENCODING STRATEGY ===\\n\")\n",
    "    \n",
    "    # 1. College_Code - Label encoding (347 unique colleges)\n",
    "    print(\"1. College_Code Encoding:\")\n",
    "    le_college_code = LabelEncoder()\n",
    "    df_encoded['College_Code_Encoded'] = le_college_code.fit_transform(df_encoded['College_Code'])\n",
    "    df_encoded = df_encoded.drop('College_Code', axis=1)\n",
    "    encoders['college_code'] = le_college_code\n",
    "    print(f\"   Label encoded College_Code -> College_Code_Encoded\")\n",
    "    print(f\"   Unique colleges: {len(le_college_code.classes_)}\")\n",
    "    \n",
    "    # 2. College_Name - Drop (redundant with College_Code)\n",
    "    print(\"\\n2. College_Name Encoding:\")\n",
    "    df_encoded = df_encoded.drop('College_Name', axis=1)\n",
    "    print(\"   Dropped College_Name (redundant with College_Code)\")\n",
    "    \n",
    "    # 3. Original Category - Label encoding (too many unique detailed categories for ordinal)\n",
    "    print(\"\\n3. Category Encoding (Label):\")\n",
    "    le_category = LabelEncoder()\n",
    "    df_encoded['Category_Encoded'] = le_category.fit_transform(df_encoded['Category'])\n",
    "    df_encoded = df_encoded.drop('Category', axis=1)\n",
    "    encoders['category'] = le_category\n",
    "    print(f\"   Label encoded Category -> Category_Encoded\")\n",
    "    print(f\"   Unique categories: {len(le_category.classes_)}\")\n",
    "    print(f\"   Sample category mapping: {dict(list(zip(le_category.classes_[:5], le_category.transform(le_category.classes_[:5]))))}...\")\n",
    "    \n",
    "    # 4. Category_Main - Ordinal encoding (natural hierarchy for Indian admissions)\n",
    "    print(\"\\n4. Category_Main Encoding (Ordinal):\")\n",
    "    # Hierarchy based on typical cutoff difficulty: General (hardest) -> Reserved -> ST -> Other\n",
    "    category_main_hierarchy = ['General', 'Reserved', 'ST', 'Other']\n",
    "    \n",
    "    oe_category_main = OrdinalEncoder(\n",
    "        categories=[category_main_hierarchy], \n",
    "        handle_unknown='use_encoded_value', \n",
    "        unknown_value=-1\n",
    "    )\n",
    "    df_encoded['Category_Main_Ordinal'] = oe_category_main.fit_transform(df_encoded[['Category_Main']])\n",
    "    df_encoded = df_encoded.drop('Category_Main', axis=1)\n",
    "    encoders['category_main'] = oe_category_main\n",
    "    print(f\"   Ordinal encoded Category_Main: {category_main_hierarchy}\")\n",
    "    \n",
    "    # 5. Branch - Label encoding (490 branches - too many for one-hot)\n",
    "    print(\"\\n5. Branch Encoding (Label):\")\n",
    "    le_branch = LabelEncoder()\n",
    "    df_encoded['Branch_Encoded'] = le_branch.fit_transform(df_encoded['Branch'])\n",
    "    df_encoded = df_encoded.drop('Branch', axis=1)\n",
    "    encoders['branch'] = le_branch\n",
    "    print(f\"   Label encoded Branch -> Branch_Encoded\")\n",
    "    print(f\"   Unique branches: {len(le_branch.classes_)}\")\n",
    "    print(f\"   Top 5 branches: {le_branch.classes_[:5]}\")\n",
    "    \n",
    "    # 6. Round - Keep as numeric (already properly encoded 0,1,2,3,4)\n",
    "    print(\"\\n6. Round Encoding:\")\n",
    "    print(f\"   Keeping Round as numeric (already properly encoded: {sorted(df_encoded['Round'].unique())})\")\n",
    "    print(f\"   Round distribution: {df_encoded['Round'].value_counts().sort_index().to_dict()}\")\n",
    "    \n",
    "    # 7. Exam_Type - Label encoding (CET vs COMEDK)\n",
    "    print(\"\\n7. Exam_Type Encoding:\")\n",
    "    le_exam = LabelEncoder()\n",
    "    df_encoded['Exam_Type_Encoded'] = le_exam.fit_transform(df_encoded['Exam_Type'])\n",
    "    df_encoded = df_encoded.drop('Exam_Type', axis=1)\n",
    "    encoders['exam_type'] = le_exam\n",
    "    exam_mapping = dict(zip(le_exam.classes_, le_exam.transform(le_exam.classes_)))\n",
    "    print(f\"   Label encoded Exam_Type: {exam_mapping}\")\n",
    "    \n",
    "    print(f\"\\n=== ENCODING SUMMARY ===\")\n",
    "    print(f\"Final encoded columns: {df_encoded.shape[1]}\")\n",
    "    print(f\"Final feature names: {list(df_encoded.columns)}\")\n",
    "    \n",
    "    # Show data types\n",
    "    print(f\"\\nData types after encoding:\")\n",
    "    print(df_encoded.dtypes)\n",
    "    \n",
    "    return df_encoded, encoders\n",
    "\n",
    "print(\"✓ Categorical encoding function defined for your dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "114f1801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APPLYING CATEGORICAL ENCODING:\n",
      "==================================================\n",
      "Using encoding strategy: comprehensive\n",
      "=== CATEGORICAL ENCODING STRATEGY ===\n",
      "\n",
      "1. College_Code Encoding:\n",
      "   Label encoded College_Code -> College_Code_Encoded\n",
      "   Unique colleges: 347\n",
      "\n",
      "2. College_Name Encoding:\n",
      "   Dropped College_Name (redundant with College_Code)\n",
      "\n",
      "3. Category Encoding (Label):\n",
      "   Label encoded Category -> Category_Encoded\n",
      "   Unique categories: 65\n",
      "   Sample category mapping: {'15569': 0, '16277': 1, '1G': 2, '1H': 3, '1K': 4}...\n",
      "\n",
      "4. Category_Main Encoding (Ordinal):\n",
      "   Ordinal encoded Category_Main: ['General', 'Reserved', 'ST', 'Other']\n",
      "\n",
      "5. Branch Encoding (Label):\n",
      "   Label encoded Branch -> Branch_Encoded\n",
      "   Unique branches: 490\n",
      "   Top 5 branches: ['AD Artificial Intel, Data Sc'\n",
      " 'AD-Artificial\\rIntelligence\\r& Data\\rScience'\n",
      " 'AD-Artificial\\rIntelligence &\\rData Science'\n",
      " 'AD-Artificial Intelligence & Data Science' 'AE Aeronaut.Engg']\n",
      "\n",
      "6. Round Encoding:\n",
      "   Keeping Round as numeric (already properly encoded: [0, 1, 2, 3, 4])\n",
      "   Round distribution: {0: 83344, 1: 88771, 2: 77231, 3: 58126, 4: 1673}\n",
      "\n",
      "7. Exam_Type Encoding:\n",
      "   Label encoded Exam_Type: {'CET': 0, 'COMEDK': 1}\n",
      "\n",
      "=== ENCODING SUMMARY ===\n",
      "Final encoded columns: 13\n",
      "Final feature names: ['Cutoff_Rank', 'Year', 'Round', 'Rank_Percentile', 'Years_Since_2020', 'Branch_Popularity_Score', 'College_Selectivity', 'Round_Difficulty', 'College_Code_Encoded', 'Category_Encoded', 'Category_Main_Ordinal', 'Branch_Encoded', 'Exam_Type_Encoded']\n",
      "\n",
      "Data types after encoding:\n",
      "Cutoff_Rank                float64\n",
      "Year                         int64\n",
      "Round                        int64\n",
      "Rank_Percentile            float64\n",
      "Years_Since_2020             int64\n",
      "Branch_Popularity_Score      int64\n",
      "College_Selectivity        float64\n",
      "Round_Difficulty             int64\n",
      "College_Code_Encoded         int32\n",
      "Category_Encoded             int32\n",
      "Category_Main_Ordinal      float64\n",
      "Branch_Encoded               int32\n",
      "Exam_Type_Encoded            int32\n",
      "dtype: object\n",
      "\n",
      "=== CATEGORICAL ENCODING RESULTS ===\n",
      "Encoded dataset shape: (309145, 13)\n",
      "Original features: 14\n",
      "Final features: 13\n",
      "\n",
      "First 5 rows of encoded data:\n",
      "   Cutoff_Rank  Year  Round  Rank_Percentile  Years_Since_2020  Branch_Popularity_Score  College_Selectivity  Round_Difficulty  College_Code_Encoded  Category_Encoded  Category_Main_Ordinal  Branch_Encoded  Exam_Type_Encoded\n",
      "0      63649.0  2023      4         0.742270                 3                      302              26819.0                 4                     3                42                    0.0               7                  1\n",
      "1      27873.0  2023      4         0.324862                 3                      116              26819.0                 4                     3                42                    0.0              15                  1\n",
      "2      46368.0  2023      4         0.545532                 3                      238              26819.0                 4                     3                42                    0.0              86                  1\n",
      "3      24270.0  2023      4         0.284625                 3                      102              26819.0                 4                     3                42                    0.0             118                  1\n",
      "4      20937.0  2023      4         0.243117                 3                      248              26819.0                 4                     3                42                    0.0             163                  1\n",
      "\n",
      "Final feature columns (13):\n",
      "   1. Cutoff_Rank\n",
      "   2. Year\n",
      "   3. Round\n",
      "   4. Rank_Percentile\n",
      "   5. Years_Since_2020\n",
      "   6. Branch_Popularity_Score\n",
      "   7. College_Selectivity\n",
      "   8. Round_Difficulty\n",
      "   9. College_Code_Encoded\n",
      "  10. Category_Encoded\n",
      "  11. Category_Main_Ordinal\n",
      "  12. Branch_Encoded\n",
      "  13. Exam_Type_Encoded\n",
      "\n",
      "=== ENCODERS STORED ===\n",
      "   college_code: 347 classes\n",
      "   category: 65 classes\n",
      "   category_main: OrdinalEncoder\n",
      "   branch: 490 classes\n",
      "   exam_type: 2 classes\n",
      "\n",
      "✅ Categorical encoding completed successfully!\n",
      "✅ Dataset ready for scaling and validation\n"
     ]
    }
   ],
   "source": [
    "# Apply categorical encoding to your dataset\n",
    "print(\"APPLYING CATEGORICAL ENCODING:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Choose encoding strategy\n",
    "ENCODING_STRATEGY = 'comprehensive'  # Using comprehensive for better model performance\n",
    "\n",
    "print(f\"Using encoding strategy: {ENCODING_STRATEGY}\")\n",
    "\n",
    "# Apply encoding\n",
    "df_encoded, encoders = encode_categorical_features(df_features, ENCODING_STRATEGY)\n",
    "\n",
    "print(f\"\\n=== CATEGORICAL ENCODING RESULTS ===\")\n",
    "print(f\"Encoded dataset shape: {df_encoded.shape}\")\n",
    "print(f\"Original features: {df_features.shape[1]}\")\n",
    "print(f\"Final features: {df_encoded.shape[1]}\")\n",
    "\n",
    "# Display first few rows of encoded data\n",
    "print(f\"\\nFirst 5 rows of encoded data:\")\n",
    "print(df_encoded.head())\n",
    "\n",
    "# Check final feature list\n",
    "print(f\"\\nFinal feature columns ({len(df_encoded.columns)}):\")\n",
    "for i, col in enumerate(df_encoded.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "# Save encoders info for future reference\n",
    "print(f\"\\n=== ENCODERS STORED ===\")\n",
    "for encoder_name, encoder in encoders.items():\n",
    "    if hasattr(encoder, 'classes_'):\n",
    "        print(f\"   {encoder_name}: {len(encoder.classes_)} classes\")\n",
    "    else:\n",
    "        print(f\"   {encoder_name}: {type(encoder).__name__}\")\n",
    "\n",
    "print(f\"\\n✅ Categorical encoding completed successfully!\")\n",
    "print(f\"✅ Dataset ready for scaling and validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f4d5128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA VALIDATION AND QUALITY CHECK ===\n",
      "1. Missing Values Check:\n",
      "   ✅ No missing values remaining\n",
      "\n",
      "2. Data Types and Memory Usage:\n",
      "   Dataset shape: (309145, 13)\n",
      "   Memory usage: 26567.28 KB\n",
      "   All columns are numeric: 13 features\n",
      "\n",
      "3. Infinite Values Check:\n",
      "   ✅ No infinite values found\n",
      "\n",
      "4. Outlier Detection (values > 3 std deviations):\n",
      "   Outliers found (this is normal for cutoff ranks):\n",
      "     Cutoff_Rank: 3037 outliers (0.98%)\n",
      "     College_Selectivity: 98 outliers (0.03%)\n",
      "\n",
      "5. Key Feature Statistics:\n",
      "   Cutoff_Rank:\n",
      "     Range: 18 to 274884\n",
      "     Mean: 81444.4, Std: 55388.5\n",
      "   Rank_Percentile:\n",
      "     Range: 0 to 1\n",
      "     Mean: 0.5, Std: 0.3\n",
      "   College_Selectivity:\n",
      "     Range: 292 to 162459\n",
      "     Mean: 67623.7, Std: 30384.1\n",
      "   Branch_Popularity_Score:\n",
      "     Range: 0 to 489\n",
      "     Mean: 335.3, Std: 66.8\n",
      "   College_Code_Encoded:\n",
      "     Range: 0 to 346\n",
      "     Mean: 119.0, Std: 83.6\n",
      "   Branch_Encoded:\n",
      "     Range: 0 to 489\n",
      "     Mean: 207.7, Std: 113.4\n",
      "\n",
      "6. Encoding Integrity Check:\n",
      "   College_Code_Encoded: 0 to 346 (347 unique)\n",
      "   Branch_Encoded: 0 to 489 (490 unique)\n",
      "   Category_Encoded: 0 to 64 (65 unique)\n",
      "   Exam_Type_Encoded: [0, 1] (CET=0, COMEDK=1)\n",
      "\n",
      "7. Data Distribution Summary:\n",
      "   Years: [2020, 2021, 2022, 2023, 2024]\n",
      "   Rounds: [0, 1, 2, 3, 4]\n",
      "   Exam types: {0: 300689, 1: 8456}\n",
      "\n",
      "✅ Data validation completed successfully!\n",
      "✅ Dataset is ready for feature scaling\n",
      "✅ Total features: 13\n",
      "✅ Total records: 309,145\n"
     ]
    }
   ],
   "source": [
    "# Validate the processed data\n",
    "print(\"=== DATA VALIDATION AND QUALITY CHECK ===\")\n",
    "\n",
    "# 1. Check for remaining missing values\n",
    "print(\"1. Missing Values Check:\")\n",
    "missing_vals = df_encoded.isnull().sum()\n",
    "total_missing = missing_vals.sum()\n",
    "\n",
    "if total_missing == 0:\n",
    "    print(\"   ✅ No missing values remaining\")\n",
    "else:\n",
    "    print(f\"   ⚠️ {total_missing} missing values found:\")\n",
    "    print(missing_vals[missing_vals > 0])\n",
    "\n",
    "# 2. Check data types and memory usage\n",
    "print(\"\\n2. Data Types and Memory Usage:\")\n",
    "print(f\"   Dataset shape: {df_encoded.shape}\")\n",
    "print(f\"   Memory usage: {df_encoded.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "\n",
    "# Count by data type\n",
    "numeric_cols = df_encoded.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"   All columns are numeric: {len(numeric_cols)} features\")\n",
    "\n",
    "# 3. Check for infinite or extreme values\n",
    "print(\"\\n3. Infinite Values Check:\")\n",
    "numeric_data = df_encoded.select_dtypes(include=[np.number])\n",
    "inf_check = np.isinf(numeric_data).sum()\n",
    "if inf_check.sum() == 0:\n",
    "    print(\"   ✅ No infinite values found\")\n",
    "else:\n",
    "    print(\"   ⚠️ Infinite values found:\")\n",
    "    print(inf_check[inf_check > 0])\n",
    "\n",
    "# 4. Check for extreme outliers (values beyond 3 standard deviations)\n",
    "print(\"\\n4. Outlier Detection (values > 3 std deviations):\")\n",
    "outlier_counts = {}\n",
    "for col in numeric_cols:\n",
    "    if col in ['Cutoff_Rank', 'College_Selectivity']:  # Focus on key continuous variables\n",
    "        mean_val = df_encoded[col].mean()\n",
    "        std_val = df_encoded[col].std()\n",
    "        outliers = ((df_encoded[col] - mean_val).abs() > 3 * std_val).sum()\n",
    "        if outliers > 0:\n",
    "            outlier_counts[col] = outliers\n",
    "\n",
    "if outlier_counts:\n",
    "    print(\"   Outliers found (this is normal for cutoff ranks):\")\n",
    "    for col, count in outlier_counts.items():\n",
    "        pct = (count / len(df_encoded)) * 100\n",
    "        print(f\"     {col}: {count} outliers ({pct:.2f}%)\")\n",
    "else:\n",
    "    print(\"   ✅ No extreme outliers detected\")\n",
    "\n",
    "# 5. Feature ranges and statistics\n",
    "print(\"\\n5. Key Feature Statistics:\")\n",
    "key_features = ['Cutoff_Rank', 'Rank_Percentile', 'College_Selectivity', \n",
    "                'Branch_Popularity_Score', 'College_Code_Encoded', 'Branch_Encoded']\n",
    "\n",
    "for col in key_features:\n",
    "    if col in df_encoded.columns:\n",
    "        stats = df_encoded[col].describe()\n",
    "        print(f\"   {col}:\")\n",
    "        print(f\"     Range: {stats['min']:.0f} to {stats['max']:.0f}\")\n",
    "        print(f\"     Mean: {stats['mean']:.1f}, Std: {stats['std']:.1f}\")\n",
    "\n",
    "# 6. Check encoding integrity\n",
    "print(\"\\n6. Encoding Integrity Check:\")\n",
    "print(f\"   College_Code_Encoded: 0 to {df_encoded['College_Code_Encoded'].max()} ({df_encoded['College_Code_Encoded'].nunique()} unique)\")\n",
    "print(f\"   Branch_Encoded: 0 to {df_encoded['Branch_Encoded'].max()} ({df_encoded['Branch_Encoded'].nunique()} unique)\")\n",
    "print(f\"   Category_Encoded: 0 to {df_encoded['Category_Encoded'].max()} ({df_encoded['Category_Encoded'].nunique()} unique)\")\n",
    "print(f\"   Exam_Type_Encoded: {sorted(df_encoded['Exam_Type_Encoded'].unique())} (CET=0, COMEDK=1)\")\n",
    "\n",
    "# 7. Data distribution check\n",
    "print(\"\\n7. Data Distribution Summary:\")\n",
    "print(f\"   Years: {sorted(df_encoded['Year'].unique())}\")\n",
    "print(f\"   Rounds: {sorted(df_encoded['Round'].unique())}\")\n",
    "print(f\"   Exam types: {df_encoded['Exam_Type_Encoded'].value_counts().to_dict()}\")\n",
    "\n",
    "print(f\"\\n✅ Data validation completed successfully!\")\n",
    "print(f\"✅ Dataset is ready for feature scaling\")\n",
    "print(f\"✅ Total features: {df_encoded.shape[1]}\")\n",
    "print(f\"✅ Total records: {df_encoded.shape[0]:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1935e9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEATURE SCALING AND FINAL PREPARATION ===\n",
      "Features to scale (6):\n",
      "   - Cutoff_Rank\n",
      "   - Year\n",
      "   - College_Selectivity\n",
      "   - Branch_Popularity_Score\n",
      "   - College_Code_Encoded\n",
      "   - Branch_Encoded\n",
      "\n",
      "Features NOT scaled (7):\n",
      "   - Round\n",
      "   - Rank_Percentile\n",
      "   - Years_Since_2020\n",
      "   - Round_Difficulty\n",
      "   - Category_Encoded\n",
      "   - Category_Main_Ordinal\n",
      "   - Exam_Type_Encoded\n",
      "\n",
      "✅ Feature scaling applied using StandardScaler\n",
      "\n",
      "=== SCALING RESULTS ===\n",
      "Before scaling (sample statistics):\n",
      "   Cutoff_Rank: Mean=81444.4, Std=55388.5\n",
      "   Year: Mean=2022.3, Std=1.4\n",
      "   College_Selectivity: Mean=67623.7, Std=30384.1\n",
      "\n",
      "After scaling (sample statistics):\n",
      "   Cutoff_Rank: Mean=0.000, Std=1.000\n",
      "   Year: Mean=0.000, Std=1.000\n",
      "   College_Selectivity: Mean=-0.000, Std=1.000\n",
      "\n",
      "=== FINAL PREPROCESSED DATASET ===\n",
      "Shape: (309145, 13)\n",
      "Features: ['Cutoff_Rank', 'Year', 'Round', 'Rank_Percentile', 'Years_Since_2020', 'Branch_Popularity_Score', 'College_Selectivity', 'Round_Difficulty', 'College_Code_Encoded', 'Category_Encoded', 'Category_Main_Ordinal', 'Branch_Encoded', 'Exam_Type_Encoded']\n",
      "Memory usage: 28982.47 KB\n",
      "\n",
      "Sample of final scaled data (first 3 rows):\n",
      "   Cutoff_Rank      Year  Round  Rank_Percentile  Years_Since_2020  Branch_Popularity_Score  College_Selectivity  Round_Difficulty  College_Code_Encoded  Category_Encoded  Category_Main_Ordinal  Branch_Encoded  Exam_Type_Encoded\n",
      "0    -0.321284  0.479869      4         0.742270                 3                -0.498273            -1.342965                 4             -1.387783                42                    0.0       -1.769986                  1\n",
      "1    -0.967195  0.479869      4         0.324862                 3                -3.282073            -1.342965                 4             -1.387783                42                    0.0       -1.699421                  1\n",
      "2    -0.633281  0.479869      4         0.545532                 3                -1.456140            -1.342965                 4             -1.387783                42                    0.0       -1.073156                  1\n",
      "\n",
      "✅ Preprocessing artifacts stored for model deployment\n",
      "✅ Dataset ready for machine learning!\n",
      "\n",
      "🎉 PREPROCESSING PIPELINE COMPLETED! 🎉\n",
      "📊 Final dataset: 309,145 rows × 13 features\n",
      "🔧 Features scaled: 6\n",
      "🏷️ Categorical features encoded: 6\n",
      "🚀 Ready for train/validation/test split!\n"
     ]
    }
   ],
   "source": [
    "# Prepare features for machine learning with proper scaling\n",
    "print(\"=== FEATURE SCALING AND FINAL PREPARATION ===\")\n",
    "\n",
    "# Identify numeric features that need scaling\n",
    "# Features with very different ranges should be scaled\n",
    "numeric_features_to_scale = [\n",
    "    'Cutoff_Rank',           # Range: 18 to 274,884\n",
    "    'Year',                  # Range: 2020 to 2024  \n",
    "    'College_Selectivity',   # Range: 292 to 162,459\n",
    "    'Branch_Popularity_Score', # Range: 0 to 489\n",
    "    'College_Code_Encoded',  # Range: 0 to 346\n",
    "    'Branch_Encoded'         # Range: 0 to 489\n",
    "]\n",
    "\n",
    "# Features that don't need scaling (already normalized or small range)\n",
    "features_no_scaling = [\n",
    "    'Round',                 # Range: 0 to 4\n",
    "    'Rank_Percentile',       # Already 0-1 normalized\n",
    "    'Years_Since_2020',      # Range: 0 to 4\n",
    "    'Round_Difficulty',      # Range: 0 to 4  \n",
    "    'Category_Encoded',      # Range: 0 to 64\n",
    "    'Category_Main_Ordinal', # Range: 0 to 3\n",
    "    'Exam_Type_Encoded'      # Range: 0 to 1\n",
    "]\n",
    "\n",
    "print(f\"Features to scale ({len(numeric_features_to_scale)}):\")\n",
    "for feat in numeric_features_to_scale:\n",
    "    print(f\"   - {feat}\")\n",
    "\n",
    "print(f\"\\nFeatures NOT scaled ({len(features_no_scaling)}):\")\n",
    "for feat in features_no_scaling:\n",
    "    print(f\"   - {feat}\")\n",
    "\n",
    "# Apply Standard Scaling to selected features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_scaled = df_encoded.copy()\n",
    "\n",
    "# Scale only the features that need it\n",
    "df_scaled[numeric_features_to_scale] = scaler.fit_transform(df_encoded[numeric_features_to_scale])\n",
    "\n",
    "print(f\"\\n✅ Feature scaling applied using StandardScaler\")\n",
    "\n",
    "# Show scaling effects\n",
    "print(f\"\\n=== SCALING RESULTS ===\")\n",
    "print(\"Before scaling (sample statistics):\")\n",
    "for feat in numeric_features_to_scale[:3]:  # Show first 3 features\n",
    "    print(f\"   {feat}: Mean={df_encoded[feat].mean():.1f}, Std={df_encoded[feat].std():.1f}\")\n",
    "\n",
    "print(\"\\nAfter scaling (sample statistics):\")\n",
    "for feat in numeric_features_to_scale[:3]:  # Show first 3 features  \n",
    "    print(f\"   {feat}: Mean={df_scaled[feat].mean():.3f}, Std={df_scaled[feat].std():.3f}\")\n",
    "\n",
    "# Display final results\n",
    "print(f\"\\n=== FINAL PREPROCESSED DATASET ===\")\n",
    "print(f\"Shape: {df_scaled.shape}\")\n",
    "print(f\"Features: {list(df_scaled.columns)}\")\n",
    "print(f\"Memory usage: {df_scaled.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "\n",
    "print(f\"\\nSample of final scaled data (first 3 rows):\")\n",
    "print(df_scaled.head(3))\n",
    "\n",
    "# Store preprocessing artifacts for future use\n",
    "preprocessing_artifacts = {\n",
    "    'scaler': scaler,\n",
    "    'scaled_features': numeric_features_to_scale,\n",
    "    'unscaled_features': features_no_scaling,\n",
    "    'encoders': encoders,\n",
    "    'feature_names': list(df_scaled.columns),\n",
    "    'total_records': df_scaled.shape[0],\n",
    "    'total_features': df_scaled.shape[1]\n",
    "}\n",
    "\n",
    "print(f\"\\n✅ Preprocessing artifacts stored for model deployment\")\n",
    "print(f\"✅ Dataset ready for machine learning!\")\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n🎉 PREPROCESSING PIPELINE COMPLETED! 🎉\")\n",
    "print(f\"📊 Final dataset: {df_scaled.shape[0]:,} rows × {df_scaled.shape[1]} features\")\n",
    "print(f\"🔧 Features scaled: {len(numeric_features_to_scale)}\")\n",
    "print(f\"🏷️ Categorical features encoded: 6\")\n",
    "print(f\"🚀 Ready for train/validation/test split!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32294dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXPORTING PROCESSED DATA ===\n",
      "✅ Processed data exported to: processed_admission_data.csv\n",
      "✅ Detailed preprocessing report saved to: preprocessing_summary_report.txt\n",
      "✅ Preprocessing objects saved to: preprocessing_objects.pkl\n",
      "\n",
      "============================================================\n",
      "🏆 DATA PREPROCESSING SUCCESSFULLY COMPLETED!\n",
      "============================================================\n",
      "📁 FILES CREATED:\n",
      "   1. processed_admission_data.csv - ML-ready dataset\n",
      "   2. preprocessing_summary_report.txt - Detailed report\n",
      "   3. preprocessing_objects.pkl - Encoders & scaler for deployment\n",
      "\n",
      "📊 DATASET STATISTICS:\n",
      "   • Records: 309,145\n",
      "   • Features: 13\n",
      "   • Size: 28982.5 KB\n",
      "\n",
      "🚀 NEXT STEPS:\n",
      "   1. Load processed data: pd.read_csv('processed_admission_data.csv')\n",
      "   2. Create target variable for your prediction task\n",
      "   3. Split into train/validation/test sets\n",
      "   4. Train machine learning models\n",
      "   5. Evaluate model performance\n",
      "\n",
      "✨ Your college admission prediction dataset is ready!\n"
     ]
    }
   ],
   "source": [
    "# Export the processed data for machine learning\n",
    "print(\"=== EXPORTING PROCESSED DATA ===\")\n",
    "\n",
    "# 1. Export cleaned and processed data\n",
    "output_filename = 'processed_admission_data.csv'\n",
    "df_scaled.to_csv(output_filename, index=False)\n",
    "print(f\"✅ Processed data exported to: {output_filename}\")\n",
    "\n",
    "# 2. Export preprocessing summary report (with UTF-8 encoding)\n",
    "summary_report = f\"\"\"\n",
    "=== COLLEGE ADMISSION DATA PREPROCESSING SUMMARY ===\n",
    "Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "DATASET OVERVIEW:\n",
    "- Original dataset: {df.shape[0]:,} rows × {df.shape[1]} columns\n",
    "- Final dataset: {df_scaled.shape[0]:,} rows × {df_scaled.shape[1]} features\n",
    "- Memory usage: {df_scaled.memory_usage(deep=True).sum() / 1024:.1f} KB\n",
    "\n",
    "DATA COVERAGE:\n",
    "- Years: {sorted(df['Year'].unique())}\n",
    "- Colleges: {df_encoded['College_Code_Encoded'].nunique()} unique institutions\n",
    "- Branches: {df_encoded['Branch_Encoded'].nunique()} engineering specializations  \n",
    "- Categories: {df_encoded['Category_Encoded'].nunique()} admission categories\n",
    "- Exam types: CET ({(df_encoded['Exam_Type_Encoded'] == 0).sum():,}) vs COMEDK ({(df_encoded['Exam_Type_Encoded'] == 1).sum():,})\n",
    "\n",
    "PREPROCESSING STEPS COMPLETED:\n",
    "- Missing values handled: 9,421 values imputed\n",
    "- Feature engineering: 6 new features created\n",
    "- Categorical encoding: 6 variables encoded\n",
    "- Feature scaling: 6 features standardized\n",
    "- Data validation: All checks passed\n",
    "\n",
    "FINAL FEATURES ({df_scaled.shape[1]}):\n",
    "{chr(10).join([f\"  {i+1:2d}. {col}\" for i, col in enumerate(df_scaled.columns)])}\n",
    "\n",
    "SCALED FEATURES: {', '.join(numeric_features_to_scale)}\n",
    "UNSCALED FEATURES: {', '.join(features_no_scaling)}\n",
    "\n",
    "READY FOR:\n",
    "- Train/Validation/Test split\n",
    "- Machine Learning model training\n",
    "- College admission prediction\n",
    "\"\"\"\n",
    "\n",
    "# Save summary report with UTF-8 encoding\n",
    "with open('preprocessing_summary_report.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(f\"✅ Detailed preprocessing report saved to: preprocessing_summary_report.txt\")\n",
    "\n",
    "# 3. Save preprocessing objects for future use\n",
    "import pickle\n",
    "\n",
    "preprocessing_objects = {\n",
    "    'scaler': scaler,\n",
    "    'encoders': encoders,\n",
    "    'feature_names': list(df_scaled.columns),\n",
    "    'scaled_features': numeric_features_to_scale,\n",
    "    'unscaled_features': features_no_scaling\n",
    "}\n",
    "\n",
    "with open('preprocessing_objects.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessing_objects, f)\n",
    "\n",
    "print(f\"✅ Preprocessing objects saved to: preprocessing_objects.pkl\")\n",
    "\n",
    "# 4. Display final success summary\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"🏆 DATA PREPROCESSING SUCCESSFULLY COMPLETED!\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"📁 FILES CREATED:\")\n",
    "print(f\"   1. {output_filename} - ML-ready dataset\")\n",
    "print(f\"   2. preprocessing_summary_report.txt - Detailed report\")\n",
    "print(f\"   3. preprocessing_objects.pkl - Encoders & scaler for deployment\")\n",
    "\n",
    "print(f\"\\n📊 DATASET STATISTICS:\")\n",
    "print(f\"   • Records: {df_scaled.shape[0]:,}\")\n",
    "print(f\"   • Features: {df_scaled.shape[1]}\")\n",
    "print(f\"   • Size: {df_scaled.memory_usage(deep=True).sum() / 1024:.1f} KB\")\n",
    "\n",
    "print(f\"\\n🚀 NEXT STEPS:\")\n",
    "print(f\"   1. Load processed data: pd.read_csv('{output_filename}')\")\n",
    "print(f\"   2. Create target variable for your prediction task\")\n",
    "print(f\"   3. Split into train/validation/test sets\")\n",
    "print(f\"   4. Train machine learning models\")\n",
    "print(f\"   5. Evaluate model performance\")\n",
    "\n",
    "print(f\"\\n✨ Your college admission prediction dataset is ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a3eefce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CREATING TARGET VARIABLES FOR MULTI-FACTOR FRAMEWORK ===\n",
      "Based on your project: 'Bridging Static Predictions and Dynamic Admissions'\n",
      "Working with dataset: 309,145 records × 13 features\n",
      "\n",
      "1. Creating Admission_Probability target (PRIMARY):\n",
      "   Distribution: {'Low': 123661, 'Very_Low': 92747, 'Medium': 61828, 'High': 30909}\n",
      "\n",
      "2. Creating Round_Likely_To_Get_Seat target:\n",
      "   Distribution: {1: 46367, 2: 77286, 3: 92745, 4: 92747}\n",
      "\n",
      "3. Creating College_Tier target (For recommendation ranking):\n",
      "   Distribution: {'Tier_1': 109923, 'Tier_4': 95677, 'Tier_3': 71249, 'Tier_2': 32296}\n",
      "\n",
      "4. Creating Recommendation_Strategy target:\n",
      "   Distribution: {'Conservative': 123661, 'Emergency': 93455, 'Aggressive': 76220, 'Balanced': 15809}\n",
      "\n",
      "5. Creating Will_Get_Preferred_College binary target:\n",
      "   Distribution: {0: 216408, 1: 92737} (1=Yes, 0=No)\n",
      "\n",
      "=== TARGET VARIABLES CREATED FOR YOUR PROJECT ===\n",
      "Created 5 target variables:\n",
      "   1. Admission_Probability\n",
      "   2. Round_Likely_To_Get_Seat\n",
      "   3. College_Tier\n",
      "   4. Recommendation_Strategy\n",
      "   5. Will_Get_Preferred_College\n",
      "\n",
      "Final dataset shape: (309145, 18)\n",
      "\n",
      "Sample of target variables:\n",
      "  Admission_Probability  Round_Likely_To_Get_Seat College_Tier Recommendation_Strategy  Will_Get_Preferred_College\n",
      "0              Very_Low                         4       Tier_1               Emergency                           0\n",
      "1                   Low                         2       Tier_1            Conservative                           0\n",
      "2                   Low                         3       Tier_1            Conservative                           0\n",
      "3                Medium                         2       Tier_1              Aggressive                           1\n",
      "4                Medium                         2       Tier_1              Aggressive                           1\n",
      "\n",
      "=== TARGET CORRELATIONS (For Multi-task Learning) ===\n",
      "                            Round_Likely_To_Get_Seat  Will_Get_Preferred_College\n",
      "Round_Likely_To_Get_Seat                    1.000000                   -0.784694\n",
      "Will_Get_Preferred_College                 -0.784694                    1.000000\n",
      "\n",
      "✅ Target variables created successfully!\n",
      "✅ Ready for your Multi-Factor ML Framework implementation!\n"
     ]
    }
   ],
   "source": [
    "# Create target variables for Multi-Factor ML Framework\n",
    "print(\"=== CREATING TARGET VARIABLES FOR MULTI-FACTOR FRAMEWORK ===\")\n",
    "print(\"Based on your project: 'Bridging Static Predictions and Dynamic Admissions'\")\n",
    "\n",
    "# Load the processed data\n",
    "df_targets = df_scaled.copy()\n",
    "print(f\"Working with dataset: {df_targets.shape[0]:,} records × {df_targets.shape[1]} features\")\n",
    "\n",
    "# 1. PRIMARY TARGET: Admission Probability (Multi-class)\n",
    "print(\"\\n1. Creating Admission_Probability target (PRIMARY):\")\n",
    "def calculate_admission_probability(row):\n",
    "    \"\"\"\n",
    "    Calculate admission probability based on rank percentile and college selectivity\n",
    "    This simulates real counseling dynamics\n",
    "    \"\"\"\n",
    "    rank_percentile = row['Rank_Percentile']\n",
    "    \n",
    "    # Higher rank percentile = worse rank = lower probability\n",
    "    if rank_percentile <= 0.10:  # Top 10% ranks\n",
    "        return 'High'    # 90%+ chance\n",
    "    elif rank_percentile <= 0.30:  # Top 30% ranks  \n",
    "        return 'Medium'  # 60-90% chance\n",
    "    elif rank_percentile <= 0.70:  # Top 70% ranks\n",
    "        return 'Low'     # 20-60% chance\n",
    "    else:\n",
    "        return 'Very_Low'  # <20% chance\n",
    "\n",
    "df_targets['Admission_Probability'] = df_targets.apply(calculate_admission_probability, axis=1)\n",
    "prob_dist = df_targets['Admission_Probability'].value_counts()\n",
    "print(f\"   Distribution: {prob_dist.to_dict()}\")\n",
    "\n",
    "# 2. SECONDARY TARGET: Round Prediction (For multi-round simulation)\n",
    "print(\"\\n2. Creating Round_Likely_To_Get_Seat target:\")\n",
    "def predict_likely_round(row):\n",
    "    \"\"\"\n",
    "    Predict which round student is likely to get seat based on rank and college selectivity\n",
    "    \"\"\"\n",
    "    rank_percentile = row['Rank_Percentile']\n",
    "    \n",
    "    if rank_percentile <= 0.15:\n",
    "        return 1  # Round 1 (best students)\n",
    "    elif rank_percentile <= 0.40:\n",
    "        return 2  # Round 2  \n",
    "    elif rank_percentile <= 0.70:\n",
    "        return 3  # Round 3\n",
    "    else:\n",
    "        return 4  # Round 4 (last chance)\n",
    "\n",
    "df_targets['Round_Likely_To_Get_Seat'] = df_targets.apply(predict_likely_round, axis=1)\n",
    "round_dist = df_targets['Round_Likely_To_Get_Seat'].value_counts().sort_index()\n",
    "print(f\"   Distribution: {round_dist.to_dict()}\")\n",
    "\n",
    "# 3. TERTIARY TARGET: College Tier Prediction  \n",
    "print(\"\\n3. Creating College_Tier target (For recommendation ranking):\")\n",
    "def assign_college_tier(selectivity):\n",
    "    \"\"\"\n",
    "    Assign college tier based on selectivity (lower cutoff = better tier)\n",
    "    \"\"\"\n",
    "    # Note: College_Selectivity is already scaled, so we use quartiles\n",
    "    if selectivity <= -0.5:  # Most selective colleges\n",
    "        return 'Tier_1'\n",
    "    elif selectivity <= 0:\n",
    "        return 'Tier_2'  \n",
    "    elif selectivity <= 0.5:\n",
    "        return 'Tier_3'\n",
    "    else:\n",
    "        return 'Tier_4'  # Least selective\n",
    "\n",
    "df_targets['College_Tier'] = df_targets['College_Selectivity'].apply(assign_college_tier)\n",
    "tier_dist = df_targets['College_Tier'].value_counts()\n",
    "print(f\"   Distribution: {tier_dist.to_dict()}\")\n",
    "\n",
    "# 4. QUATERNARY TARGET: Choice Filling Strategy (For \"What-if\" scenarios)\n",
    "print(\"\\n4. Creating Recommendation_Strategy target:\")\n",
    "def recommend_strategy(row):\n",
    "    \"\"\"\n",
    "    Recommend choice filling strategy based on student profile\n",
    "    \"\"\"\n",
    "    prob = row['Admission_Probability']\n",
    "    tier = row['College_Tier']\n",
    "    \n",
    "    if prob in ['High', 'Medium'] and tier in ['Tier_1', 'Tier_2']:\n",
    "        return 'Aggressive'  # Go for top colleges\n",
    "    elif prob == 'Medium':\n",
    "        return 'Balanced'    # Mix of safe and stretch choices\n",
    "    elif prob == 'Low':\n",
    "        return 'Conservative'  # Focus on safe options\n",
    "    else:\n",
    "        return 'Emergency'   # Focus on any available seat\n",
    "\n",
    "df_targets['Recommendation_Strategy'] = df_targets.apply(recommend_strategy, axis=1)\n",
    "strategy_dist = df_targets['Recommendation_Strategy'].value_counts()\n",
    "print(f\"   Distribution: {strategy_dist.to_dict()}\")\n",
    "\n",
    "# 5. BINARY TARGET: Will_Get_Preferred_College (For confidence scoring)\n",
    "print(\"\\n5. Creating Will_Get_Preferred_College binary target:\")\n",
    "df_targets['Will_Get_Preferred_College'] = (\n",
    "    df_targets['Admission_Probability'].isin(['High', 'Medium'])\n",
    ").astype(int)\n",
    "\n",
    "binary_dist = df_targets['Will_Get_Preferred_College'].value_counts()\n",
    "print(f\"   Distribution: {binary_dist.to_dict()} (1=Yes, 0=No)\")\n",
    "\n",
    "print(f\"\\n=== TARGET VARIABLES CREATED FOR YOUR PROJECT ===\")\n",
    "target_columns = ['Admission_Probability', 'Round_Likely_To_Get_Seat', 'College_Tier', \n",
    "                 'Recommendation_Strategy', 'Will_Get_Preferred_College']\n",
    "\n",
    "print(f\"Created {len(target_columns)} target variables:\")\n",
    "for i, target in enumerate(target_columns, 1):\n",
    "    print(f\"   {i}. {target}\")\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {df_targets.shape}\")\n",
    "print(f\"\\nSample of target variables:\")\n",
    "print(df_targets[target_columns].head())\n",
    "\n",
    "# Show correlation between targets (for multi-task learning)\n",
    "print(f\"\\n=== TARGET CORRELATIONS (For Multi-task Learning) ===\")\n",
    "# Create correlation matrix for numeric targets\n",
    "numeric_targets = ['Round_Likely_To_Get_Seat', 'Will_Get_Preferred_College']\n",
    "if len(numeric_targets) > 1:\n",
    "    target_corr = df_targets[numeric_targets].corr()\n",
    "    print(target_corr)\n",
    "\n",
    "print(f\"\\n✅ Target variables created successfully!\")\n",
    "print(f\"✅ Ready for your Multi-Factor ML Framework implementation!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3078276d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAIN/VALIDATION/TEST SPLIT FOR MULTI-FACTOR FRAMEWORK ===\n",
      "Final dataset shape: (309145, 18)\n",
      "Features (X): (309145, 13)\n",
      "Feature names: ['Cutoff_Rank', 'Year', 'Round', 'Rank_Percentile', 'Years_Since_2020', 'Branch_Popularity_Score', 'College_Selectivity', 'Round_Difficulty', 'College_Code_Encoded', 'Category_Encoded', 'Category_Main_Ordinal', 'Branch_Encoded', 'Exam_Type_Encoded']\n",
      "\n",
      "1. Admission_Probability encoded:\n",
      "   Classes: {'High': 0, 'Low': 1, 'Medium': 2, 'Very_Low': 3}\n",
      "\n",
      "2. Round_Likely_To_Get_Seat (numeric): Range 1 to 4\n",
      "\n",
      "3. College_Tier encoded:\n",
      "   Classes: {'Tier_1': 0, 'Tier_2': 1, 'Tier_3': 2, 'Tier_4': 3}\n",
      "\n",
      "4. Recommendation_Strategy encoded:\n",
      "   Classes: {'Aggressive': 0, 'Balanced': 1, 'Conservative': 2, 'Emergency': 3}\n",
      "\n",
      "5. Will_Get_Preferred_College (binary): (array([0, 1]), array([216408,  92737], dtype=int64))\n",
      "\n",
      "=== PERFORMING STRATIFIED SPLIT ===\n",
      "Using Admission_Probability for stratification to ensure balanced splits\n",
      "Train+Validation: 247,316 samples\n",
      "Test: 61,829 samples\n",
      "Final split:\n",
      "  Training: 185,487 samples (60.0%)\n",
      "  Validation: 61,829 samples (20.0%)\n",
      "  Test: 61,829 samples (20.0%)\n",
      "\n",
      "=== VERIFYING STRATIFIED SPLIT ===\n",
      "Train distribution: {0: 10.0, 1: 40.0, 2: 20.0, 3: 30.0}\n",
      "Validation distribution: {0: 10.0, 1: 40.0, 2: 20.0, 3: 30.0}\n",
      "Test distribution: {0: 10.0, 1: 40.0, 2: 20.0, 3: 30.0}\n",
      "\n",
      "✅ Stratified split completed successfully!\n",
      "✅ All target variables split consistently\n",
      "✅ Ready for Multi-Factor ML Framework training!\n",
      "\n",
      "=== DATASETS READY FOR YOUR PROJECT MODELS ===\n",
      "🎯 XGBoost: Use X_train, y_train_dict['admission_prob'] for primary model\n",
      "🌲 Random Forest: Use for ensemble with different targets\n",
      "📊 LSTM: Use temporal features (Year, Round) for trend analysis\n",
      "🔄 Multi-task Learning: Train on multiple targets simultaneously\n"
     ]
    }
   ],
   "source": [
    "# Train/Validation/Test Split optimized for Multi-Factor ML Framework\n",
    "print(\"=== TRAIN/VALIDATION/TEST SPLIT FOR MULTI-FACTOR FRAMEWORK ===\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Prepare the final dataset\n",
    "print(f\"Final dataset shape: {df_targets.shape}\")\n",
    "\n",
    "# Separate features from targets\n",
    "feature_columns = ['Cutoff_Rank', 'Year', 'Round', 'Rank_Percentile', 'Years_Since_2020',\n",
    "                  'Branch_Popularity_Score', 'College_Selectivity', 'Round_Difficulty',\n",
    "                  'College_Code_Encoded', 'Category_Encoded', 'Category_Main_Ordinal', \n",
    "                  'Branch_Encoded', 'Exam_Type_Encoded']\n",
    "\n",
    "target_columns = ['Admission_Probability', 'Round_Likely_To_Get_Seat', 'College_Tier',\n",
    "                 'Recommendation_Strategy', 'Will_Get_Preferred_College']\n",
    "\n",
    "X = df_targets[feature_columns]\n",
    "y_dict = {}\n",
    "\n",
    "print(f\"Features (X): {X.shape}\")\n",
    "print(f\"Feature names: {list(X.columns)}\")\n",
    "\n",
    "# Encode categorical targets for ML\n",
    "target_encoders = {}\n",
    "\n",
    "# 1. Primary target: Admission_Probability (4 classes)\n",
    "le_admission = LabelEncoder()\n",
    "y_dict['admission_prob'] = le_admission.fit_transform(df_targets['Admission_Probability'])\n",
    "target_encoders['admission_prob'] = le_admission\n",
    "print(f\"\\n1. Admission_Probability encoded:\")\n",
    "print(f\"   Classes: {dict(zip(le_admission.classes_, le_admission.transform(le_admission.classes_)))}\")\n",
    "\n",
    "# 2. Round prediction (already numeric)\n",
    "y_dict['round_prediction'] = df_targets['Round_Likely_To_Get_Seat'].values\n",
    "print(f\"\\n2. Round_Likely_To_Get_Seat (numeric): Range {y_dict['round_prediction'].min()} to {y_dict['round_prediction'].max()}\")\n",
    "\n",
    "# 3. College tier \n",
    "le_tier = LabelEncoder()\n",
    "y_dict['college_tier'] = le_tier.fit_transform(df_targets['College_Tier'])\n",
    "target_encoders['college_tier'] = le_tier\n",
    "print(f\"\\n3. College_Tier encoded:\")\n",
    "print(f\"   Classes: {dict(zip(le_tier.classes_, le_tier.transform(le_tier.classes_)))}\")\n",
    "\n",
    "# 4. Recommendation strategy\n",
    "le_strategy = LabelEncoder()\n",
    "y_dict['recommendation_strategy'] = le_strategy.fit_transform(df_targets['Recommendation_Strategy'])\n",
    "target_encoders['recommendation_strategy'] = le_strategy\n",
    "print(f\"\\n4. Recommendation_Strategy encoded:\")\n",
    "print(f\"   Classes: {dict(zip(le_strategy.classes_, le_strategy.transform(le_strategy.classes_)))}\")\n",
    "\n",
    "# 5. Binary target (already numeric)\n",
    "y_dict['binary_preferred'] = df_targets['Will_Get_Preferred_College'].values\n",
    "print(f\"\\n5. Will_Get_Preferred_College (binary): {np.unique(y_dict['binary_preferred'], return_counts=True)}\")\n",
    "\n",
    "# Perform stratified split using primary target (Admission_Probability)\n",
    "print(f\"\\n=== PERFORMING STRATIFIED SPLIT ===\")\n",
    "print(\"Using Admission_Probability for stratification to ensure balanced splits\")\n",
    "\n",
    "# First split: 80% train+val, 20% test\n",
    "X_temp, X_test, y_temp_dict, y_test_dict = {}, {}, {}, {}\n",
    "\n",
    "# Split based on primary target\n",
    "temp_indices, test_indices = train_test_split(\n",
    "    range(len(X)), \n",
    "    test_size=0.2, \n",
    "    stratify=y_dict['admission_prob'], \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_temp = X.iloc[temp_indices]\n",
    "X_test = X.iloc[test_indices]\n",
    "\n",
    "for target_name, target_values in y_dict.items():\n",
    "    y_temp_dict[target_name] = target_values[temp_indices]\n",
    "    y_test_dict[target_name] = target_values[test_indices]\n",
    "\n",
    "print(f\"Train+Validation: {X_temp.shape[0]:,} samples\")\n",
    "print(f\"Test: {X_test.shape[0]:,} samples\")\n",
    "\n",
    "# Second split: 75% train, 25% validation (from temp)\n",
    "train_indices, val_indices = train_test_split(\n",
    "    range(len(X_temp)), \n",
    "    test_size=0.25, \n",
    "    stratify=y_temp_dict['admission_prob'], \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train = X_temp.iloc[train_indices]\n",
    "X_val = X_temp.iloc[val_indices]\n",
    "\n",
    "y_train_dict, y_val_dict = {}, {}\n",
    "for target_name, target_values in y_temp_dict.items():\n",
    "    y_train_dict[target_name] = target_values[train_indices]\n",
    "    y_val_dict[target_name] = target_values[val_indices]\n",
    "\n",
    "print(f\"Final split:\")\n",
    "print(f\"  Training: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  Validation: {X_val.shape[0]:,} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  Test: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Verify stratification worked\n",
    "print(f\"\\n=== VERIFYING STRATIFIED SPLIT ===\")\n",
    "for split_name, y_split in [(\"Train\", y_train_dict['admission_prob']), \n",
    "                           (\"Validation\", y_val_dict['admission_prob']), \n",
    "                           (\"Test\", y_test_dict['admission_prob'])]:\n",
    "    unique, counts = np.unique(y_split, return_counts=True)\n",
    "    percentages = counts / len(y_split) * 100\n",
    "    print(f\"{split_name} distribution: {dict(zip(unique, percentages.round(1)))}\")\n",
    "\n",
    "print(f\"\\n✅ Stratified split completed successfully!\")\n",
    "print(f\"✅ All target variables split consistently\")\n",
    "print(f\"✅ Ready for Multi-Factor ML Framework training!\")\n",
    "\n",
    "# Store split information\n",
    "split_info = {\n",
    "    'train_size': len(X_train),\n",
    "    'val_size': len(X_val), \n",
    "    'test_size': len(X_test),\n",
    "    'feature_names': list(X.columns),\n",
    "    'target_names': list(y_dict.keys()),\n",
    "    'target_encoders': target_encoders\n",
    "}\n",
    "\n",
    "print(f\"\\n=== DATASETS READY FOR YOUR PROJECT MODELS ===\")\n",
    "print(f\"🎯 XGBoost: Use X_train, y_train_dict['admission_prob'] for primary model\")\n",
    "print(f\"🌲 Random Forest: Use for ensemble with different targets\")\n",
    "print(f\"📊 LSTM: Use temporal features (Year, Round) for trend analysis\")\n",
    "print(f\"🔄 Multi-task Learning: Train on multiple targets simultaneously\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c51d0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXPORTING MULTI-FACTOR ML FRAMEWORK DATASET ===\n",
      "1. Creating comprehensive dataset exports...\n",
      "   Master dataset shape: (309145, 19)\n",
      "   Columns: ['Cutoff_Rank', 'Year', 'Round', 'Rank_Percentile', 'Years_Since_2020', 'Branch_Popularity_Score', 'College_Selectivity', 'Round_Difficulty', 'College_Code_Encoded', 'Category_Encoded', 'Category_Main_Ordinal', 'Branch_Encoded', 'Exam_Type_Encoded', 'target_admission_prob', 'target_round_prediction', 'target_college_tier', 'target_recommendation_strategy', 'target_binary_preferred', 'dataset_split']\n",
      "\n",
      "2. Exporting individual split files...\n",
      "   ✅ X_train.csv: (185487, 13)\n",
      "   ✅ X_val.csv: (61829, 13)\n",
      "   ✅ X_test.csv: (61829, 13)\n",
      "   ✅ train_complete.csv: (185487, 19)\n",
      "   ✅ val_complete.csv: (61829, 19)\n",
      "   ✅ test_complete.csv: (61829, 19)\n",
      "   ✅ master_ml_dataset.csv: (309145, 19)\n",
      "\n",
      "3. Exporting targets for multi-task learning...\n",
      "   Target files created for: ['admission_prob', 'round_prediction', 'college_tier', 'recommendation_strategy', 'binary_preferred']\n",
      "\n",
      "4. Saving preprocessing artifacts...\n",
      "   ✅ complete_ml_artifacts.pkl saved\n",
      "\n",
      "5. Creating final project report...\n",
      "   ✅ FINAL_PROJECT_REPORT.txt created\n",
      "\n",
      "================================================================================\n",
      "🏆 MULTI-FACTOR ML FRAMEWORK DATASET COMPLETED! 🏆\n",
      "================================================================================\n",
      "📊 YOUR PROJECT IS READY:\n",
      "   • Dataset: 309,145 records with 13 features\n",
      "   • Targets: 5 sophisticated target variables\n",
      "   • Models: Ready for XGBoost, Random Forest, LSTM integration\n",
      "   • Framework: Multi-factor prediction with confidence scoring\n",
      "\n",
      "📁 KEY FILES FOR YOUR PROJECT:\n",
      "   🎯 master_ml_dataset.csv - Your complete ML dataset\n",
      "   🔧 complete_ml_artifacts.pkl - All preprocessing objects\n",
      "   📋 FINAL_PROJECT_REPORT.txt - Comprehensive documentation\n",
      "\n",
      "🚀 START MODEL DEVELOPMENT WITH:\n",
      "   df = pd.read_csv('master_ml_dataset.csv')\n",
      "   # Begin with XGBoost on Admission_Probability target\n",
      "\n",
      "✨ Your Multi-Factor ML Framework for College Admission is ready! ✨\n"
     ]
    }
   ],
   "source": [
    "# Export the complete Multi-Factor ML Framework dataset\n",
    "print(\"=== EXPORTING MULTI-FACTOR ML FRAMEWORK DATASET ===\")\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. Create comprehensive dataset exports\n",
    "print(\"1. Creating comprehensive dataset exports...\")\n",
    "\n",
    "# Combine all data for export\n",
    "def create_export_dataset(X_data, y_data_dict, split_name):\n",
    "    \"\"\"Create a complete dataset with features and all targets\"\"\"\n",
    "    export_df = X_data.copy()\n",
    "    \n",
    "    # Add all target columns\n",
    "    for target_name, target_values in y_data_dict.items():\n",
    "        export_df[f'target_{target_name}'] = target_values\n",
    "    \n",
    "    # Add split identifier\n",
    "    export_df['dataset_split'] = split_name\n",
    "    \n",
    "    return export_df\n",
    "\n",
    "# Create complete datasets for each split\n",
    "train_complete = create_export_dataset(X_train, y_train_dict, 'train')\n",
    "val_complete = create_export_dataset(X_val, y_val_dict, 'validation')  \n",
    "test_complete = create_export_dataset(X_test, y_test_dict, 'test')\n",
    "\n",
    "# Combine all splits into one master dataset\n",
    "master_dataset = pd.concat([train_complete, val_complete, test_complete], ignore_index=True)\n",
    "\n",
    "print(f\"   Master dataset shape: {master_dataset.shape}\")\n",
    "print(f\"   Columns: {list(master_dataset.columns)}\")\n",
    "\n",
    "# 2. Export individual split files\n",
    "print(f\"\\n2. Exporting individual split files...\")\n",
    "\n",
    "# Features only\n",
    "X_train.to_csv('X_train.csv', index=False)\n",
    "X_val.to_csv('X_val.csv', index=False)\n",
    "X_test.to_csv('X_test.csv', index=False)\n",
    "\n",
    "# Complete datasets (features + targets)\n",
    "train_complete.to_csv('train_complete.csv', index=False)\n",
    "val_complete.to_csv('val_complete.csv', index=False)\n",
    "test_complete.to_csv('test_complete.csv', index=False)\n",
    "\n",
    "# Master dataset\n",
    "master_dataset.to_csv('master_ml_dataset.csv', index=False)\n",
    "\n",
    "print(f\"   ✅ X_train.csv: {X_train.shape}\")\n",
    "print(f\"   ✅ X_val.csv: {X_val.shape}\")  \n",
    "print(f\"   ✅ X_test.csv: {X_test.shape}\")\n",
    "print(f\"   ✅ train_complete.csv: {train_complete.shape}\")\n",
    "print(f\"   ✅ val_complete.csv: {val_complete.shape}\")\n",
    "print(f\"   ✅ test_complete.csv: {test_complete.shape}\")\n",
    "print(f\"   ✅ master_ml_dataset.csv: {master_dataset.shape}\")\n",
    "\n",
    "# 3. Export targets separately for multi-task learning\n",
    "print(f\"\\n3. Exporting targets for multi-task learning...\")\n",
    "\n",
    "for target_name in y_train_dict.keys():\n",
    "    # Export each target separately\n",
    "    pd.Series(y_train_dict[target_name], name=target_name).to_csv(f'y_train_{target_name}.csv', index=False)\n",
    "    pd.Series(y_val_dict[target_name], name=target_name).to_csv(f'y_val_{target_name}.csv', index=False)\n",
    "    pd.Series(y_test_dict[target_name], name=target_name).to_csv(f'y_test_{target_name}.csv', index=False)\n",
    "\n",
    "print(f\"   Target files created for: {list(y_train_dict.keys())}\")\n",
    "\n",
    "# 4. Save all preprocessing artifacts and metadata\n",
    "print(f\"\\n4. Saving preprocessing artifacts...\")\n",
    "\n",
    "# Complete preprocessing pipeline\n",
    "complete_artifacts = {\n",
    "    # Original preprocessing\n",
    "    'feature_scaler': scaler,\n",
    "    'encoders': encoders,\n",
    "    'target_encoders': target_encoders,\n",
    "    \n",
    "    # Dataset information\n",
    "    'feature_names': list(X.columns),\n",
    "    'target_names': list(y_dict.keys()),\n",
    "    'split_info': split_info,\n",
    "    \n",
    "    # Dataset sizes\n",
    "    'train_size': len(X_train),\n",
    "    'val_size': len(X_val),\n",
    "    'test_size': len(X_test),\n",
    "    \n",
    "    # Feature engineering info\n",
    "    'scaled_features': numeric_features_to_scale,\n",
    "    'unscaled_features': features_no_scaling,\n",
    "    \n",
    "    # Target information\n",
    "    'target_distributions': {\n",
    "        'admission_prob_classes': dict(zip(le_admission.classes_, le_admission.transform(le_admission.classes_))),\n",
    "        'college_tier_classes': dict(zip(le_tier.classes_, le_tier.transform(le_tier.classes_))),\n",
    "        'strategy_classes': dict(zip(le_strategy.classes_, le_strategy.transform(le_strategy.classes_)))\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save artifacts\n",
    "with open('complete_ml_artifacts.pkl', 'wb') as f:\n",
    "    pickle.dump(complete_artifacts, f)\n",
    "\n",
    "print(f\"   ✅ complete_ml_artifacts.pkl saved\")\n",
    "\n",
    "# 5. Create detailed project report\n",
    "print(f\"\\n5. Creating final project report...\")\n",
    "\n",
    "project_report = f\"\"\"\n",
    "=====================================================================================\n",
    "MULTI-FACTOR ML FRAMEWORK FOR COLLEGE ADMISSION PREDICTION - DATASET READY\n",
    "=====================================================================================\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Project: \"Bridging Static Predictions and Dynamic Admissions\"\n",
    "\n",
    "📊 DATASET OVERVIEW:\n",
    "• Total Records: {master_dataset.shape[0]:,}\n",
    "• Features: {len(list(X.columns))}\n",
    "• Target Variables: {len(list(y_dict.keys()))}\n",
    "• Years Covered: 2020-2024\n",
    "• Institutions: 347 colleges\n",
    "• Branches: 490 engineering specializations\n",
    "• Categories: 65 admission categories\n",
    "\n",
    "🎯 TARGET VARIABLES CREATED:\n",
    "1. Admission_Probability (4 classes): High, Medium, Low, Very_Low\n",
    "2. Round_Likely_To_Get_Seat (numeric): Rounds 1-4\n",
    "3. College_Tier (4 classes): Tier_1, Tier_2, Tier_3, Tier_4  \n",
    "4. Recommendation_Strategy (4 classes): Aggressive, Balanced, Conservative, Emergency\n",
    "5. Will_Get_Preferred_College (binary): 0/1\n",
    "\n",
    "📈 DATA SPLITS (STRATIFIED):\n",
    "• Training: {len(X_train):,} samples (60.0%)\n",
    "• Validation: {len(X_val):,} samples (20.0%)\n",
    "• Testing: {len(X_test):,} samples (20.0%)\n",
    "\n",
    "🔧 FEATURES ENGINEERED:\n",
    "• Rank_Percentile: Normalized ranking within year/exam\n",
    "• Branch_Popularity_Score: Branch competitiveness ranking\n",
    "• College_Selectivity: Institution selectivity metrics\n",
    "• Temporal Features: Years_Since_2020, Round_Difficulty\n",
    "• Encoded Categories: College, Branch, Category, Exam_Type\n",
    "\n",
    "📁 FILES GENERATED:\n",
    "Dataset Files:\n",
    "• master_ml_dataset.csv - Complete dataset with all targets\n",
    "• X_train.csv, X_val.csv, X_test.csv - Feature splits\n",
    "• train_complete.csv, val_complete.csv, test_complete.csv - Complete splits\n",
    "• y_train_*.csv, y_val_*.csv, y_test_*.csv - Individual target files\n",
    "\n",
    "Artifacts:\n",
    "• complete_ml_artifacts.pkl - All preprocessing objects\n",
    "• preprocessing_objects.pkl - Basic preprocessing\n",
    "• processed_admission_data.csv - Original processed data\n",
    "\n",
    "🚀 READY FOR YOUR PROJECT MODELS:\n",
    "\n",
    "1. XGBoost Primary Model:\n",
    "   - Input: X_train, y_train_dict['admission_prob']\n",
    "   - Task: Multi-class classification (4 classes)\n",
    "   - Expected Accuracy: 85%+\n",
    "\n",
    "2. Random Forest Ensemble:\n",
    "   - Input: All target combinations\n",
    "   - Task: Multi-task learning\n",
    "   - Purpose: Confidence scoring\n",
    "\n",
    "3. LSTM Temporal Model:\n",
    "   - Input: Temporal features (Year, Round)\n",
    "   - Task: Trend analysis and forecasting\n",
    "   - Purpose: Dynamic admission patterns\n",
    "\n",
    "4. Multi-Factor Framework:\n",
    "   - Combine all models for comprehensive predictions\n",
    "   - \"What-if\" scenario testing\n",
    "   - Round-wise probability updates\n",
    "   - Personalized recommendations\n",
    "\n",
    "💡 MODEL DEVELOPMENT PRIORITY:\n",
    "Phase 1: XGBoost for Admission_Probability (PRIMARY TARGET)\n",
    "Phase 2: Random Forest for ensemble learning\n",
    "Phase 3: Multi-task learning with all targets\n",
    "Phase 4: LSTM integration for temporal patterns\n",
    "Phase 5: Complete Multi-Factor Framework\n",
    "\n",
    "✅ PHASE 1 COMPLETED: Dataset Collection & Cleaning\n",
    "🎯 NEXT: Phase 2 - Multi-Factor ML Model Development\n",
    "\n",
    "Your dataset is production-ready for sophisticated college admission prediction!\n",
    "=====================================================================================\n",
    "\"\"\"\n",
    "\n",
    "with open('FINAL_PROJECT_REPORT.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(project_report)\n",
    "\n",
    "print(f\"   ✅ FINAL_PROJECT_REPORT.txt created\")\n",
    "\n",
    "# 6. Final success summary\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"🏆 MULTI-FACTOR ML FRAMEWORK DATASET COMPLETED! 🏆\")\n",
    "print(f\"=\"*80)\n",
    "\n",
    "print(f\"📊 YOUR PROJECT IS READY:\")\n",
    "print(f\"   • Dataset: {master_dataset.shape[0]:,} records with {len(list(X.columns))} features\")\n",
    "print(f\"   • Targets: {len(list(y_dict.keys()))} sophisticated target variables\")\n",
    "print(f\"   • Models: Ready for XGBoost, Random Forest, LSTM integration\")\n",
    "print(f\"   • Framework: Multi-factor prediction with confidence scoring\")\n",
    "\n",
    "print(f\"\\n📁 KEY FILES FOR YOUR PROJECT:\")\n",
    "print(f\"   🎯 master_ml_dataset.csv - Your complete ML dataset\")\n",
    "print(f\"   🔧 complete_ml_artifacts.pkl - All preprocessing objects\")\n",
    "print(f\"   📋 FINAL_PROJECT_REPORT.txt - Comprehensive documentation\")\n",
    "\n",
    "print(f\"\\n🚀 START MODEL DEVELOPMENT WITH:\")\n",
    "print(f\"   df = pd.read_csv('master_ml_dataset.csv')\")\n",
    "print(f\"   # Begin with XGBoost on Admission_Probability target\")\n",
    "\n",
    "print(f\"\\n✨ Your Multi-Factor ML Framework for College Admission is ready! ✨\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kcet-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
